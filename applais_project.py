# -*- coding: utf-8 -*-
"""Applais project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VP6s-nsEkDvoJAkuCvUMUjN4CCrv6F8z

# **Preprocessing**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv('Applais project.csv')
df.info()

df.head()

df.isnull().sum()

tmp = df['mobile_wt'].dropna(axis=0)
mean = tmp.mean()
print(mean)
df['mobile_wt'].fillna(mean, inplace=True)

df.describe()

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
scaled = pd.DataFrame(sc.fit_transform(df))
scaled.columns = df.columns
df = scaled
df.head()



"""# **K-Means**"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from yellowbrick.cluster import KElbowVisualizer

kmeans = KMeans()
ssd = []
K = range(1, 30)

for k in K:
    kmeans = KMeans(n_clusters=k).fit(df)
    ssd.append(kmeans.inertia_)

ssd

plt.plot(K, ssd, "bx-")
plt.xlabel("Distance Residual Sums for K Values (WCSS)")
plt.title("Elbow Method for Optimum Number of Clusters")
plt.show()

kmeans = KMeans()
visu = KElbowVisualizer(kmeans, k=(2, 20))
visu.fit(df)
visu.show()

kmeans = KMeans(n_clusters=7).fit(df)
clusters = kmeans.labels_

df['clusters'] = clusters
df.head()

df.groupby("clusters").agg({"clusters": "count"})

import pickle

pickle.dump(kmeans, open('KMeans.sav', 'wb'))

"""# **Multinomial Logistic Regression**

**Feature Selection**
"""

cor = df.corr()
target = abs(cor['clusters'])
features = target[target >= 0.25]
df = df[features.index]
df.shape

df.columns

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from numpy import mean
from numpy import std

X = df.loc[:'three_g']
y = df['clusters']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

model = LogisticRegression(multi_class='multinomial', max_iter=1000)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

pickle.dump(model, open('LogisticRegression.sav', 'wb'))